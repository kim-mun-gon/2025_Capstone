import os
import re
import pickle
import joblib
import unicodedata
import requests
import numpy as np
import pandas as pd
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from contextlib import asynccontextmanager
from sentence_transformers import SentenceTransformer, util
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import MinMaxScaler
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, ElementClickInterceptedException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import torch
from torch import nn
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from konlpy.tag import Okt
import torch.nn.functional as F
from kiwipiepy import Kiwi
from keybert import KeyBERT
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch.nn.functional as F
import json

# app.py ìµœìƒë‹¨ì— í•œë²ˆë§Œ ì„ ì–¸
kiwi     = Kiwi()
kw_model = KeyBERT()

okt = Okt()

# ===== ì „ì—­ ìºì‹œ ë° ëª¨ë¸ ë¡œë”© =====
naver_ranking_cache, teen_ranking_cache = [], []
twenty_ranking_cache, thirty_ranking_cache, forty_ranking_cache = [], [], []

# ìœ ì‚¬ë„ ëª¨ë¸
similarity_model = SentenceTransformer("nlpai-lab/KURE-v1")
with open("./models/ì›¹ì†Œì„¤_ìœ ì‚¬ë„_ë¶„ì„ëª¨ë¸_í†µí•©.pkl", "rb") as f:
    embedding_package = pickle.load(f)

# ê°ì„± ë¶„ì„ íŒŒì´í”„ë¼ì¸ ë¡œë”©
# ìƒˆë¡œìš´ ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ë¡œë“œ
# 1) KR-BERT íŒŒì´í”„ë¼ì¸ë§Œ ë¡œë“œ
with open("models/krbert_pipeline.pkl", "rb") as f:
    krbert_pipe = pickle.load(f)

# 2) HuggingFace AutoTokenizer ìƒì„±
#    krbert_pipe["model_name"] ì— ì €ì¥ëœ ëª¨ë¸ ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
tokenizer = AutoTokenizer.from_pretrained(krbert_pipe["model_name"])
max_len   = krbert_pipe["max_len"]

# 3) PyTorchìš© KR-BERT ë¶„ë¥˜ ëª¨ë¸ ë¡œë”©
sentiment_model = AutoModelForSequenceClassification.from_pretrained(
    krbert_pipe["model_name"],
    num_labels=2
)
sentiment_model.load_state_dict(
    torch.load("models/krbert_dropout_30.pt", map_location="cpu")
)
sentiment_model.eval()

# ì„¤ì • ë¡œë“œ
with open("models/impact_scorer_ver3.pkl", "rb") as f:
    cfg = pickle.load(f)

motifs        = cfg["motif_list"]
turn_keywords = cfg["turn_keywords"]
senti_dict    = cfg["sentiword_dict"]

def tokenize(text: str):
    if not text:
        return []
    return [t.form for t in kiwi.analyze(text)[0][0]]

def emotion_score(tokens):
    n = max(len(tokens), 1)
    hits = [w for w in tokens if senti_dict.get(w, 0) != 0]
    return min(len(hits) / n, 1.0), hits

def motif_score(text: str):
    n = max(len(motifs), 1)
    hits = [m for m in motifs if m in text]
    return min(len(hits) / n, 1.0), hits

def transition_score(text: str):
    n = max(len(turn_keywords), 1)
    hits = [kw for kw in turn_keywords if kw in text]
    return min(len(hits) / n, 1.0), hits



# ì„±ê³µ ì˜ˆì¸¡ ëª¨ë¸ ë¡œë“œ
xgb_model = joblib.load("models/kure_xgb_total_model.pkl")
embed_model = SentenceTransformer("nlpai-lab/KURE-v1")

# ===== ìœ í‹¸ í•¨ìˆ˜ =====
def create_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=options)

def get_tags():
    res = requests.get("https://novel.naver.com/webnovel/weekday", headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(res.text, "html.parser")
    return [btn.get_text(strip=True) for btn in soup.select("button.tag")]

def get_naver_webnovel_rankings():
    res = requests.get("https://novel.naver.com/webnovel/weekday", headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(res.text, "html.parser")
    data = []
    for item in soup.select("li.item")[:15]:
        try:
            data.append({
                "title":  item.select_one("span.title").text.strip(),
                "author": item.select_one("span.author").text.strip(),
                "genre":  item.select_one("span.genre").text.strip(),
                "count":  item.select_one("span.count").text.strip(),
                "image":  item.select_one("img")["src"],
                "link":   "https://novel.naver.com" + item.select_one("a")["href"]
            })
        except:
            continue
    return data

def get_age_ranking(age_label):
    driver = create_driver()
    try:
        driver.get("https://novel.naver.com/webnovel/weekday")
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "#agePopularTabPanel .button_tab"))
        )
        tabs = driver.find_elements(By.CSS_SELECTOR, "#agePopularTabPanel .button_tab")
        for tab in tabs:
            if tab.get_attribute("innerText").strip() == age_label:
                driver.execute_script("arguments[0].click();", tab)
                break
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "#agePopularRanking li.item"))
        )
        soup = BeautifulSoup(driver.page_source, "html.parser")
        data = []
        for item in soup.select("#agePopularRanking li.item")[:15]:
            try:
                data.append({
                    "title":  item.select_one("span.title").text.strip(),
                    "author": item.select_one("span.author").text.strip(),
                    "genre":  item.select_one("span.genre").text.strip(),
                    "count":  item.select_one("span.count").text.strip(),
                    "image":  item.select_one("img")["src"],
                    "link":   "https://novel.naver.com" + item.select_one("a")["href"]
                })
            except:
                continue
        return data
    except:
        return []
    finally:
        driver.quit()

def update_all_rankings():
    global naver_ranking_cache, teen_ranking_cache
    global twenty_ranking_cache, thirty_ranking_cache, forty_ranking_cache

    naver_ranking_cache  = get_naver_webnovel_rankings()
    teen_ranking_cache   = get_age_ranking("10ëŒ€")
    twenty_ranking_cache = get_age_ranking("20ëŒ€")
    thirty_ranking_cache = get_age_ranking("30ëŒ€")
    forty_ranking_cache  = get_age_ranking("40ëŒ€")

def parse_count(text):
    text = re.sub(r"[ê°€-í£\s,]*", "", str(text))
    m = re.search(r"(\d+)(ë§Œ)?", text)
    if not m:
        return 0
    return int(m.group(1)) * 10000 if m.group(2) else int(m.group(1))

@asynccontextmanager
async def lifespan(app: FastAPI):
    update_all_rankings()
    yield

app = FastAPI(lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

class SummaryRequest(BaseModel):
    title: str
    genre: str
    summary: str

class PredictRequest(BaseModel):
    title: str
    summary: str
    genre: str

class SuccessRequest(BaseModel):
    title: str
    genre: str
    summary: str

@app.get("/api/tags")
async def tags():
    return JSONResponse(content=get_tags())

@app.get("/api/naver-rankings")
async def naver_rankings():
    return JSONResponse(content=naver_ranking_cache)

@app.get("/api/rankings/teen")
async def teen_rankings():
    return JSONResponse(content=teen_ranking_cache)

@app.get("/api/rankings/twenty")
async def twenty_rankings():
    return JSONResponse(content=twenty_ranking_cache)

@app.get("/api/rankings/thirty")
async def thirty_rankings():
    return JSONResponse(content=thirty_ranking_cache)

@app.get("/api/rankings/forty")
async def forty_rankings():
    return JSONResponse(content=forty_ranking_cache)

@app.post("/api/similarity")
def get_similar_novels(req: SummaryRequest):
    genre_norm = unicodedata.normalize("NFC", req.genre.strip())
    if genre_norm not in embedding_package:
        return {"error": f"'{req.genre}' ì¥ë¥´ ë°ì´í„° ì—†ìŒ"}

    vec    = similarity_model.encode(f"{req.title} {req.summary}")
    data   = embedding_package[genre_norm]
    scores = util.cos_sim(vec, data["embeddings"])[0]
    idxs   = np.argsort(-scores)[:10]

    similar_titles = [data["titles"][i] for i in idxs]

    return {
        "similar_novels": [
            {
                "title":      data["titles"][i],
                "summary":    data["summaries"][i],
                "similarity": f"{scores[i].item()*100:.1f}%"
            }
            for i in idxs
        ],
        "similar_titles": similar_titles
    }

@app.post("/api/predict-score")
def predict_score(req: PredictRequest):
    genre_norm = unicodedata.normalize("NFC", req.genre.strip())
    if genre_norm not in embedding_package:
        return {"error": f"'{req.genre}' ì¥ë¥´ ë°ì´í„° ì—†ìŒ"}

    vec    = similarity_model.encode(f"{req.title} {req.summary}")
    data   = embedding_package[genre_norm]
    scores = util.cos_sim(vec, data["embeddings"])[0]
    idxs   = np.argsort(-scores)[:10]
    similar_titles = [data["titles"][i] for i in idxs]

    info_path    = f"webnovel_data/{genre_norm}_ê¸°ë³¸ì •ë³´_ì¤„ê±°ë¦¬í¬í•¨.csv"
    comment_path = f"webnovel_data/{genre_norm}_ëŒ“ê¸€.csv"
    if not os.path.exists(info_path) or not os.path.exists(comment_path):
        return {"error": "í•„ìš”í•œ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}

    df = pd.read_csv(info_path)
    df = df[df["ì œëª©"].isin(similar_titles)]
    comments = pd.read_csv(comment_path)
    comments = comments[similar_titles]

    df["ë‹¤ìš´ë¡œë“œ ìˆ˜"] = df["ë‹¤ìš´ë¡œë“œ ìˆ˜"].apply(parse_count)
    df["ê´€ì‹¬ ìˆ˜"]   = df["ê´€ì‹¬ ìˆ˜"].apply(parse_count)

    from text_preprocessor import clean_text, tokenize_and_remove_stopwords

    pos, neg = {}, {}
    for col in comments.columns:
        texts = comments[col].dropna().tolist()
        if not texts:
            pos[col] = neg[col] = np.nan
            continue

        # 1) ì „ì²˜ë¦¬
        cleaned = [clean_text(t) for t in texts]

        # 2) HuggingFace í† í¬ë‚˜ì´ì €ë¡œ ë°°ì¹˜ ì¸ì½”ë”©
        enc = tokenizer(
            cleaned,
            padding="max_length",
            truncation=True,
            max_length=max_len,
            return_tensors="pt"
        )

        # 3) BERT inference
        with torch.no_grad():
            outputs = sentiment_model(**enc)
            probs   = F.softmax(outputs.logits, dim=1).cpu().numpy()

        # 4) ê¸ì • í´ë˜ìŠ¤(preds[:,1]) ê¸°ì¤€ìœ¼ë¡œ ì´ì§„í™”
        preds = (probs[:, 1] >= 0.5).astype(int)

        # 5) ë¹„ìœ¨ ê³„ì‚°
        total     = len(preds)
        pos[col]  = round(preds.sum() / total, 5)
        neg[col]  = round((total - preds.sum()) / total, 5)

    df["ê¸ì • ë¹„ìœ¨"] = df["ì œëª©"].map(pos)
    df["ë¶€ì • ë¹„ìœ¨"] = df["ì œëª©"].map(neg)
    df = df.dropna(subset=["ê¸ì • ë¹„ìœ¨", "ë¶€ì • ë¹„ìœ¨", "ë³„ì "])
    if df.empty:
        return {"error": "ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

    avg_positive = df["ê¸ì • ë¹„ìœ¨"].mean()
    w = {"ë‹¤ìš´ë¡œë“œ ìˆ˜":0.1, "ê´€ì‹¬ ìˆ˜":0.25, "ê¸ì • ë¹„ìœ¨":0.35, "ë¶€ì • ë¹„ìœ¨":0.2, "ë³„ì ":0.1}
    norm = lambda x: x / x.max()
    weighted = (
        norm(df["ë‹¤ìš´ë¡œë“œ ìˆ˜"]).mean() * w["ë‹¤ìš´ë¡œë“œ ìˆ˜"]
      + norm(df["ê´€ì‹¬ ìˆ˜"]).mean()   * w["ê´€ì‹¬ ìˆ˜"]
      + df["ê¸ì • ë¹„ìœ¨"].mean()       * w["ê¸ì • ë¹„ìœ¨"]
      + df["ë¶€ì • ë¹„ìœ¨"].mean()       * w["ë¶€ì • ë¹„ìœ¨"]
      + (df["ë³„ì "].mean() / 10)     * w["ë³„ì "]
    ) * 10

    Xs = MinMaxScaler().fit_transform(df[["ë‹¤ìš´ë¡œë“œ ìˆ˜","ê´€ì‹¬ ìˆ˜","ê¸ì • ë¹„ìœ¨","ë¶€ì • ë¹„ìœ¨","ë³„ì "]])
    rf = RandomForestRegressor(random_state=42)
    rf.fit(Xs, df["ë³„ì "])
    ml_pred = rf.predict([Xs.mean(axis=0)])[0]

    alpha = 0.7 if len(df) < 10 else 0.5 if len(df) < 20 else 0.3
    hybrid = weighted * alpha + ml_pred * (1 - alpha)

    return {
        "predicted_rating": round(hybrid, 2),
        "positive_rate":     round(avg_positive, 5)
    }

@app.post("/api/success-predict")
def success_predict(req: SuccessRequest):
    # 1) ìœ ì‚¬ì‘í’ˆ 5ê°œ ì¶”ì¶œ
    genre_norm = unicodedata.normalize("NFC", req.genre.strip())
    if genre_norm not in embedding_package:
        return {"error": "ì¥ë¥´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

    # similarity_model + embedding_package ì—ì„œ titles, embeddings ë¶ˆëŸ¬ì˜¤ê¸°
    vec_scores = similarity_model.encode(f"{req.title} {req.summary}")
    data       = embedding_package[genre_norm]
    scores     = util.cos_sim(vec_scores, data["embeddings"])[0]
    idxs       = np.argsort(-scores)[:5]
    similar_titles = [data["titles"][i] for i in idxs]

    # 2) ë°ì´í„° ë¡œë“œ & ìœ ì‚¬ì‘í’ˆ í•„í„°ë§
    info_path = f"webnovel_data/{genre_norm}_ê¸°ë³¸ì •ë³´_ì¤„ê±°ë¦¬í¬í•¨.csv"
    if not os.path.exists(info_path):
        return {"error": "ì¥ë¥´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

    df     = pd.read_csv(info_path)
    df_sim = df[df["ì œëª©"].isin(similar_titles)]
    if df_sim.empty:
        return {"error": "ì¶©ë¶„í•œ ìœ ì‚¬ì‘ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

    # 3) ìœ ì‚¬ì‘ ê¸°ë°˜ ì§€í‘œ ê³„ì‚°
    sim_avg    = df_sim["ë³„ì "].mean()
    dl_avg     = df_sim["ë‹¤ìš´ë¡œë“œ ìˆ˜"] \
                     .apply(lambda x: float(re.sub(r"[^\d.]", "", str(x)) or 0)) \
                     .mean()
    it_avg     = df_sim["ê´€ì‹¬ ìˆ˜"] \
                     .apply(lambda x: int(re.sub(r"[^\d]", "", str(x)) or 0)) \
                     .mean()
    genre_code = pd.Series([req.genre]).astype("category").cat.codes[0]

    # 4) XGBoost ì…ë ¥ ë²¡í„° ì¤€ë¹„ (1-D numpy arrays)
    # 4-1) ì¤„ê±°ë¦¬ ì„ë² ë”© ë²¡í„°
    embed_vec = embed_model.encode([req.summary])
    embed_vec = np.array(embed_vec[0], dtype=float)   # (dim,)

    # 4-2) í”¼ì³ ë²¡í„°
    feat_vec  = np.array([dl_avg, it_avg, genre_code], dtype=float)  # (3,)

    # 5) ìµœì¢… ì…ë ¥ ë²¡í„° ê²°í•©
    input_vec  = np.concatenate((embed_vec, feat_vec))  # (dim+3,)

    # 6) ì˜ˆì¸¡ í‰ì  ì‚°ì¶œ
    exp_rating  = float(xgb_model.predict([input_vec])[0])

    # 7) ì„±ê³µ í™•ë¥  ê°€ì¤‘ í‰ê·  (ëª¨ë‘ ìœ ì‚¬ì‘ ê¸°ë°˜)
    success_prob = exp_rating * 0.5 + sim_avg * 0.3 + sim_avg * 0.2

    # 8) ë“±ê¸‰ ë§¤í•‘
    if success_prob >= 9.5:
        grade = "S"
    elif success_prob >= 9.0:
        grade = "A"
    elif success_prob >= 8.0:
        grade = "B"
    elif success_prob >= 7.0:
        grade = "C"
    else:
        grade = "D"

    # 9) ì •ì„± í”¼ë“œë°±
    feedback = []
    if exp_rating < 6.5:
        feedback.append("ğŸ› ï¸ ìŠ¤í† ë¦¬ ê°œì—°ì„± ë° ëª°ì…ë„ë¥¼ ê°•í™”í•´ë³´ì„¸ìš”.")
    if exp_rating > 9.0 and sim_avg < 8.0:
        feedback.append("ğŸ§ª ì‹¤í—˜ì ì¸ êµ¬ì„± ê³ ë ¤: ì‹œì¥ ê²€ì¦ì´ í•„ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤.")
    if sim_avg >= 9.2 and exp_rating < sim_avg - 1:
        feedback.append("ğŸ“Š ìœ ì‚¬ì‘ í‰ê·  í‰ì ì´ ë†’ì•„ ê²½ìŸì´ ì¹˜ì—´í•©ë‹ˆë‹¤. ì°¨ë³„í™”ëœ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    if hasattr(req, "positive_rate") and req.positive_rate < 0.4:
        feedback.append("ğŸ—¨ï¸ ìœ ì‚¬ ì‘í’ˆë“¤ì˜ ëŒ“ê¸€ì—ì„œ ë¶€ì •ì  ì˜ê²¬ì´ ë§ìŠµë‹ˆë‹¤. êµ¬ì„± ê°œì„ ì„ ê²€í† í•´ë³´ì„¸ìš”.")
    if not feedback:
        feedback.append("âœ… ê¸°ë³¸ ìš”ì†Œ ì–‘í˜¸í•˜ë‚˜, ë…ì°½ì  ì „ê°œë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”.")

    return {
        "success_probability": round(success_prob, 2),
        "grade":               grade,
        "feedback":            feedback
    }



@app.post("/api/novelty-score")
def get_novelty_score(req: SummaryRequest):
    import pickle

    # 1) ëª¨ë¸ ë°ì´í„° ë¡œë“œ (ì¥ë¥´ í‚¤ì›Œë“œ, ë¶ˆìš©ì–´)
    with open("models/novelty_model_v5.pkl", "rb") as f:
        model_data = pickle.load(f)
    genre_keywords = model_data["genre_keywords"]
    stopwords      = set(model_data["stopwords"])

    # 2) í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
    def extract_keywords(text, top_n=100):
        allowed_pos = {"NNG", "NNP"}
        tokens = [
            token for token, pos, _, _
            in kiwi.analyze(text)[0][0]
            if pos in allowed_pos and len(token) > 1 and token not in stopwords
        ]
        joined = " ".join(tokens)
        kws = kw_model.extract_keywords(
            joined,
            keyphrase_ngram_range=(1, 1),
            use_mmr=True,
            diversity=0.5,
            top_n=top_n
        )
        return [kw for kw, _ in kws]

    # 3) ì‚¬ìš©ì í‚¤ì›Œë“œ & ì¥ë¥´ í‚¤ì›Œë“œ ë¹„êµ
    user_kws = extract_keywords(req.summary)
    user_set = set(user_kws)
    genre_set = set(genre_keywords.get(req.genre, []))

    overlap_ratio = len(user_set & genre_set) / (len(user_set) or 1)
    score = round((1 - overlap_ratio) * 100, 1)

    return {
        "novelty_score": score,
        "user_keywords": user_kws,
        "overlap_keywords": list(user_set & genre_set),
        "overlap_ratio": f"{round(overlap_ratio*100,1)}%"
    }


@app.post("/api/impact-score")
def get_impact_score(req: SummaryRequest):
    text = req.summary or ""

    # 1) í† í°í™” ë° ê° ìš”ì†Œ íˆíŠ¸
    tokens    = tokenize(text)
    _, emo_h  = emotion_score(tokens)
    _, mot_h  = motif_score(text)
    _, trn_h  = transition_score(text)

    # 2) ìµœì¢… ì„íŒ©íŠ¸ ì ìˆ˜ (ë¹„ìœ¨ì€ ë‚´ë¶€ë§Œ ì‚¬ìš©)
    emo_s, _  = emotion_score(tokens)
    mot_s, _  = motif_score(text)
    trn_s, _  = transition_score(text)
    impact_score = round((emo_s * 0.4 + mot_s * 0.3 + trn_s * 0.3) * 10, 2)

    # 3) ë°˜í™˜
    return {
        "impact_score":    impact_score,
        "emotion_hits":    emo_h,
        "motif_hits":      mot_h,
        "transition_hits": trn_h
    }




if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app:app", host="0.0.0.0", port=5001, reload=True)
